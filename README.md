<img src="http://imgur.com/1ZcRyrc.png" style="float: left; margin: 20px; height: 55px">

# Capstone Project - DeFakeIt : Detecting Deepfake Audio

>Author: Gilbert

---

## **Context:**

The rapid advancement of artificial intelligence (AI) technology has revolutionized audio generation, enabling the creation of remarkably realistic synthetic speech. This innovation holds tremendous potential for enhancing accessibility, language translation, and entertainment, benefiting individuals and industries worldwide. However, the same AI capabilities have been exploited for malicious purposes, leading to the proliferation of deepfake audio used in scams, misinformation campaigns, and hate speech. The ease with which AI-generated audio can impersonate individuals and manipulate recordings has raised serious concerns about authenticity and trust in digital communication channels.

This has been a problem in Singapore as well, with recent deepfake video  circulating online impersonating PM Lee voice to promote cryptocurrency investment. [Source : Straits Times Dec 2023](https://www.straitstimes.com/singapore/pm-lee-warns-against-responding-to-deepfake-videos-of-him-promoting-investment-scams)

## **Problem Statement:**  

How can we develop a model to effectively detect deepfake audio recordings, distinguishing between genuine human speech and AI generated sound for ensuring audio authenticity and combating the spread of misinformation and fraudulent activities?

## **Data Source**

The audio files are downloaded from the following source: 
1. [LJ Speech](https://keithito.com/LJ-Speech-Dataset/) 

 A public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.

These data are labeled as `real` audio.

2. [WaveFake: A data set to facilitate audio DeepFake detection](https://zenodo.org/records/5642694)

The audio files from `LJ Speech` are imported into 6 different types of architecture and generated audio files with identical phrases are created from each architecture. The 6 architecture are
- MelGAN
- Parallel WaveGAN
- Multi-Band MelGAN
- Full-Band MelGAN
- HiFi GAN
- WaveGlow

In total, there are 78,600 generated audio files. These data are labeled as `fake` audio.

The datasets can be downloaded from the following link: [Dataset](https://drive.google.com/drive/folders/1FjfS55cwUL182PKCGfFcnAH9vTyzRC1H?usp=sharing)

## Classification Model

Two different modelling approach are attempted, `conventional machine learning model` and `deep learning`

The conventional ML model attempted are as follows: 
1. Random Forest Classifier
2. Logistic Regression
3. XGBoost Classifier

Summary of Train and Test Score

Accuracy: 

| Model                   | Train Score | Test Score |
|-------------------------|-------------|------------|
| Random Forest Classifier| 0.749       | 0.632      |
| Logistic Regression     | 0.688       | 0.676      |
| XG Boost                | 0.857       | 0.690      |

Recall: 

| Model                   | Train Score | Test Score |
|-------------------------|-------------|------------|
| Random Forest Classifier| 0.780       | 0.655      |
| Logistic Regression     | 0.689       | 0.676      |
| XG Boost                | 0.869       | 0.732      |


Deep learning method attempeted is using the combination of `Convolutional Neural Network(CNN) + Long Short Term Memory` architecture.

Accuracy: 

| Model                   | Train Score | Test Score |
|-------------------------|-------------|------------|
| CNN + LSTM              | 0.970       | 0.928      |


Recall: 

| Model                   | Train Score | Test Score |
|-------------------------|-------------|------------|
| CNN + LSTM              | 0.967       | 0.927      | 

`CNN + LSTM` is chosen as the prediction model due to its higher accuracy and recall score compared to conventional machine learning model


## **Limitation** 

These are the limitations that are identified from the model:
1. The model might not perform well when applied to audio generated by a new AI system that differs significantly from those used during model training.
2. The model's effectiveness may be diminished when detecting audio in environments with significant background noise.
3. The model might not be effective in detecting audio at varying volumes.

## **Conclusion**

A classification model built with combination of `Convolutional Neural Network(CNN) + Long Short Term Memory` has been created, which could distinguish if the audio is `real` or `fake`. A proof of concept created can give a prediction of the audio file that is uploaded into it. Having this model could alert the community if the audio that they received is `real` or `fake`.

### Future Work and Recommendation

The next step is to work with SPF Scam Division web developer team to implement this chatbot to their webpage. The purpose is so that user can use that platform to upload the audio files that they wish to check if it is `real` or `fake`. 

To address the limitation and improve the model robustness, the following steps will be taken:
1. Collecting data from potentially new types of deepfake audio generator models and incorporating them into the training data.
2. Augment the dataset by introducing various types and levels of background noise to simulate real-world environments. 
3. Collect speech data at different volumes and train the model with this diverse dataset.






