{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project - DeFakeIt : Detecting Deepfake Audio\n",
    "\n",
    ">Author: Gilbert\n",
    "\n",
    "---\n",
    "\n",
    "**Context:**\n",
    "\n",
    "The rapid advancement of artificial intelligence (AI) technology has revolutionized audio generation, enabling the creation of remarkably realistic synthetic speech. This innovation holds tremendous potential for enhancing accessibility, language translation, and entertainment, benefiting individuals and industries worldwide. However, the same AI capabilities have been exploited for malicious purposes, leading to the proliferation of deepfake audio used in scams, misinformation campaigns, and hate speech. The ease with which AI-generated audio can impersonate individuals and manipulate recordings has raised serious concerns about authenticity and trust in digital communication channels.\n",
    "\n",
    "This has been a problem in Singapore as well, with recent deepfake video  circulating online impersonating PM Lee voice to promote cryptocurrency investment. [Source : Straits Times Dec 2023](https://www.straitstimes.com/singapore/pm-lee-warns-against-responding-to-deepfake-videos-of-him-promoting-investment-scams)\n",
    "\n",
    "**Problem Statement:**  \n",
    "\n",
    "How can we develop a model to effectively detect deepfake audio recordings, distinguishing between genuine human speech and AI generated sound for ensuring audio authenticity and combating the spread of misinformation and fraudulent activities?\n",
    "\n",
    "**Target Audience:**  \n",
    "\n",
    "SPF Scam Division  \n",
    "\n",
    "These are the notebooks for this project:  \n",
    " 1. [`01 Feature Engineering and EDA`](01_feature_engineering_and_EDA.ipynb)\n",
    " 2. [`02 Baseline Modelling`](02_baseline_modelling.ipynb)\n",
    " 3. [`03 Hyperparameter Tuning of Baseline Model`](03_hyperparametertuning_traditional_model.ipynb)\n",
    " 4. [`04 Deep Learning Modelling`](04_deep_learning_modelling.ipynb)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    " # This Notebook: 02_Traditional_Classifier_Modelling\n",
    "\n",
    "In this notebook, we will create various of baseline classifier model and evaluate the top 3 best models to be further hypertuned on the next part of the notebook\n",
    "\n",
    "To determine the appropriate baseline models, we initially evaluate both interpretability and performance based on the graph. Interpretability entails how easily one can understand and explain a model's predictions, whereas performance assesses the model's ability to capture data patterns effectively and generalize to new instances.\n",
    "\n",
    "The table below compares the pros and cons of the different classification models:\n",
    "\n",
    "| Classification Method           | Pros                                     | Cons                                                       | Usage Suggestions                                                                                                                  |\n",
    "|--------------------------------|------------------------------------------|------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Logistic Regression             | - Interpretable coefficients             | - Assumes linear relationship                             | Use for straightforward interpretation of how each feature influences the risk of chronic diseases. Suitable for cases where linear relationships between features and outcome are plausible. |\n",
    "| Decision Trees                  | - Easy to interpret and visualize       | - Prone to overfitting                                    | Use for initial exploration of feature importance and identification of relevant predictors. Prune the tree to prevent overfitting. Suitable for both numerical and categorical data. |\n",
    "| Random Forest                   | - Reduces overfitting                   | - Less interpretable than Decision Trees                  | Use for improved generalization by combining multiple decision trees. Utilize feature importance measures to understand which lifestyle factors contribute most to the risk of chronic diseases. Suitable for large datasets. |\n",
    "| Extra Trees                     | - Reduces variance further              | - Sacrifices interpretability for improved performance   | Use for faster training and potentially better performance compared to Random Forest. Particularly useful when computational resources are limited, and interpretability is not the primary concern. |\n",
    "| Support Vector Machines (SVM)   | - Effective in high-dimensional spaces | - Complexity in choosing the appropriate kernel          | Use for finding optimal hyperplanes to separate high-risk and low-risk individuals. Requires careful selection of hyperparameters and choice of kernel function. Suitable for cases with complex, non-linear relationships. |\n",
    "| k-Nearest Neighbors (k-NN)     | - Simple and intuitive                  | - Sensitive to irrelevant features                        | Use for identifying high-risk individuals based on similarity to other high-risk cases in the dataset. Normalize features and tune the number of neighbors to improve performance. Suitable for small to medium-sized datasets. |\n",
    "| Naive Bayes                     | - Computationally efficient            | - Assumes strong independence between features            | Use for quick classification of high-risk individuals based on conditional probabilities. Suitable for cases with categorical features and where independence assumptions are not severely violated. |\n",
    "| Gradient Boosting Machines (GBM)| - Combines weak learners to improve accuracy | - Can be computationally expensive and prone to overfitting | Use for building a strong predictive model by sequentially correcting errors of weak models. Regularize hyperparameters to prevent overfitting. Suitable for datasets with complex relationships and high predictive accuracy requirements. |\n",
    "| AdaBoost                        | - Sequentially combines weak learners   | - Sensitive to noisy data                                 | Use for iteratively adjusting weights to focus on previously misclassified cases. Prune weak learners to improve generalization. Suitable for ensemble learning when there's a large imbalance between high-risk and low-risk individuals. |\n",
    "| XGBoost                         | - High performance and scalability     | - Less interpretable than simpler models                  | Use for maximizing predictive accuracy and handling large datasets. Tune hyperparameters to balance bias and variance. Suitable for situations where interpretability is less critical compared to predictive power. |\n",
    "\n",
    "With the consideration of interpretability and performance, below are the selected 6 baseline model with good interpretability and performance.\n",
    "\n",
    "| Classifier                   | Interpretability | Performance | Recommendations                                                                                                                                                                  |\n",
    "|------------------------------|------------------|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Logistic Regression          | High             | Moderate    | Suitable for linearly separable data, easy to interpret coefficients, works well with small to medium-sized datasets.                                                  |\n",
    "| Random Forest                | Moderate         | High        | Combines multiple decision trees to reduce overfitting, robust to noise and outliers, suitable for large datasets with high dimensionality.                            |\n",
    "| Support Vector Machines (SVM)| Low              | High        | Effective in high-dimensional spaces, versatile due to different kernel functions, can be memory intensive, suitable for small to medium-sized datasets.            |\n",
    "| Gradient Boosting Machines (GBM)| Low           | High        | Ensemble method that combines weak learners to improve accuracy, less interpretable due to complexity, suitable for various types of data.                            |\n",
    "| XGBoost                      | Low              | High        | Optimized implementation of gradient boosting, often outperforms other algorithms, less interpretable but highly accurate, suitable for large datasets.               |\n",
    "|Decision Trees\t               |Moderate\t      |Moderate\t    | Simple to understand and visualize, prone to overfitting with complex datasets, suitable for small to medium-sized datasets when used in ensemble methods.                |\n",
    "\n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Libraries and Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "audio_df = pd.read_csv('../data/audio_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Overview of the datasets**\n",
    "\n",
    "---\n",
    "\n",
    "This is done to have an overview of how the datasets look like and what the content is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>label</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>...</th>\n",
       "      <th>chroma_feature_4</th>\n",
       "      <th>chroma_feature_5</th>\n",
       "      <th>chroma_feature_6</th>\n",
       "      <th>chroma_feature_7</th>\n",
       "      <th>chroma_feature_8</th>\n",
       "      <th>chroma_feature_9</th>\n",
       "      <th>chroma_feature_10</th>\n",
       "      <th>chroma_feature_11</th>\n",
       "      <th>chroma_feature_12</th>\n",
       "      <th>chroma_feature_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.089659</td>\n",
       "      <td>1304.235799</td>\n",
       "      <td>-323.98456</td>\n",
       "      <td>110.490395</td>\n",
       "      <td>-15.569651</td>\n",
       "      <td>29.687305</td>\n",
       "      <td>-9.337172</td>\n",
       "      <td>-10.990068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321698</td>\n",
       "      <td>0.316995</td>\n",
       "      <td>0.316548</td>\n",
       "      <td>0.302425</td>\n",
       "      <td>0.274523</td>\n",
       "      <td>0.287040</td>\n",
       "      <td>0.296070</td>\n",
       "      <td>0.311789</td>\n",
       "      <td>0.331806</td>\n",
       "      <td>0.378438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.039649</td>\n",
       "      <td>799.118275</td>\n",
       "      <td>-343.70306</td>\n",
       "      <td>160.342900</td>\n",
       "      <td>-12.456814</td>\n",
       "      <td>16.461798</td>\n",
       "      <td>-1.550739</td>\n",
       "      <td>-12.002090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472099</td>\n",
       "      <td>0.447077</td>\n",
       "      <td>0.368103</td>\n",
       "      <td>0.304562</td>\n",
       "      <td>0.384273</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>0.224810</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>0.254745</td>\n",
       "      <td>0.259224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.093555</td>\n",
       "      <td>1297.138552</td>\n",
       "      <td>-311.03366</td>\n",
       "      <td>106.231544</td>\n",
       "      <td>-10.309275</td>\n",
       "      <td>24.133608</td>\n",
       "      <td>-11.460187</td>\n",
       "      <td>-11.904459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306986</td>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.292850</td>\n",
       "      <td>0.265886</td>\n",
       "      <td>0.319642</td>\n",
       "      <td>0.348089</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.355265</td>\n",
       "      <td>0.398820</td>\n",
       "      <td>0.364525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.089239</td>\n",
       "      <td>1316.380153</td>\n",
       "      <td>-353.74420</td>\n",
       "      <td>115.892930</td>\n",
       "      <td>-23.654213</td>\n",
       "      <td>33.060036</td>\n",
       "      <td>-12.907932</td>\n",
       "      <td>-9.533757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331827</td>\n",
       "      <td>0.310453</td>\n",
       "      <td>0.318776</td>\n",
       "      <td>0.314396</td>\n",
       "      <td>0.279102</td>\n",
       "      <td>0.254030</td>\n",
       "      <td>0.211233</td>\n",
       "      <td>0.242437</td>\n",
       "      <td>0.328217</td>\n",
       "      <td>0.382869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...</td>\n",
       "      <td>real</td>\n",
       "      <td>0.072209</td>\n",
       "      <td>1167.977057</td>\n",
       "      <td>-338.63590</td>\n",
       "      <td>114.528320</td>\n",
       "      <td>-10.806395</td>\n",
       "      <td>34.839474</td>\n",
       "      <td>-0.268747</td>\n",
       "      <td>-7.760354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299801</td>\n",
       "      <td>0.330489</td>\n",
       "      <td>0.353679</td>\n",
       "      <td>0.359427</td>\n",
       "      <td>0.333285</td>\n",
       "      <td>0.280020</td>\n",
       "      <td>0.237893</td>\n",
       "      <td>0.272207</td>\n",
       "      <td>0.336213</td>\n",
       "      <td>0.351948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_file label  \\\n",
       "0  c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...  real   \n",
       "1  c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...  real   \n",
       "2  c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...  real   \n",
       "3  c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...  real   \n",
       "4  c:\\Users\\User\\GA\\sandbox\\capstone_project\\data...  real   \n",
       "\n",
       "   zero_crossing_rate  spectral_centroid     mfcc_1      mfcc_2     mfcc_3  \\\n",
       "0            0.089659        1304.235799 -323.98456  110.490395 -15.569651   \n",
       "1            0.039649         799.118275 -343.70306  160.342900 -12.456814   \n",
       "2            0.093555        1297.138552 -311.03366  106.231544 -10.309275   \n",
       "3            0.089239        1316.380153 -353.74420  115.892930 -23.654213   \n",
       "4            0.072209        1167.977057 -338.63590  114.528320 -10.806395   \n",
       "\n",
       "      mfcc_4     mfcc_5     mfcc_6  ...  chroma_feature_4  chroma_feature_5  \\\n",
       "0  29.687305  -9.337172 -10.990068  ...          0.321698          0.316995   \n",
       "1  16.461798  -1.550739 -12.002090  ...          0.472099          0.447077   \n",
       "2  24.133608 -11.460187 -11.904459  ...          0.306986          0.329119   \n",
       "3  33.060036 -12.907932  -9.533757  ...          0.331827          0.310453   \n",
       "4  34.839474  -0.268747  -7.760354  ...          0.299801          0.330489   \n",
       "\n",
       "   chroma_feature_6  chroma_feature_7  chroma_feature_8  chroma_feature_9  \\\n",
       "0          0.316548          0.302425          0.274523          0.287040   \n",
       "1          0.368103          0.304562          0.384273          0.362903   \n",
       "2          0.292850          0.265886          0.319642          0.348089   \n",
       "3          0.318776          0.314396          0.279102          0.254030   \n",
       "4          0.353679          0.359427          0.333285          0.280020   \n",
       "\n",
       "   chroma_feature_10  chroma_feature_11  chroma_feature_12  chroma_feature_13  \n",
       "0           0.296070           0.311789           0.331806           0.378438  \n",
       "1           0.224810           0.212382           0.254745           0.259224  \n",
       "2           0.311742           0.355265           0.398820           0.364525  \n",
       "3           0.211233           0.242437           0.328217           0.382869  \n",
       "4           0.237893           0.272207           0.336213           0.351948  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91700 entries, 0 to 91699\n",
      "Data columns (total 43 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   audio_file          91700 non-null  object \n",
      " 1   label               91700 non-null  object \n",
      " 2   zero_crossing_rate  91700 non-null  float64\n",
      " 3   spectral_centroid   91700 non-null  float64\n",
      " 4   mfcc_1              91700 non-null  float64\n",
      " 5   mfcc_2              91700 non-null  float64\n",
      " 6   mfcc_3              91700 non-null  float64\n",
      " 7   mfcc_4              91700 non-null  float64\n",
      " 8   mfcc_5              91700 non-null  float64\n",
      " 9   mfcc_6              91700 non-null  float64\n",
      " 10  mfcc_7              91700 non-null  float64\n",
      " 11  mfcc_8              91700 non-null  float64\n",
      " 12  mfcc_9              91700 non-null  float64\n",
      " 13  mfcc_10             91700 non-null  float64\n",
      " 14  mfcc_11             91700 non-null  float64\n",
      " 15  mfcc_12             91700 non-null  float64\n",
      " 16  mfcc_13             91700 non-null  float64\n",
      " 17  d_mfcc_1            91700 non-null  float64\n",
      " 18  d_mfcc_2            91700 non-null  float64\n",
      " 19  d_mfcc_3            91700 non-null  float64\n",
      " 20  d_mfcc_4            91700 non-null  float64\n",
      " 21  d_mfcc_5            91700 non-null  float64\n",
      " 22  d_mfcc_6            91700 non-null  float64\n",
      " 23  d_mfcc_7            91700 non-null  float64\n",
      " 24  d_mfcc_8            91700 non-null  float64\n",
      " 25  d_mfcc_9            91700 non-null  float64\n",
      " 26  d_mfcc_10           91700 non-null  float64\n",
      " 27  d_mfcc_11           91700 non-null  float64\n",
      " 28  d_mfcc_12           91700 non-null  float64\n",
      " 29  d_mfcc_13           91700 non-null  float64\n",
      " 30  chroma_feature_1    91700 non-null  float64\n",
      " 31  chroma_feature_2    91700 non-null  float64\n",
      " 32  chroma_feature_3    91700 non-null  float64\n",
      " 33  chroma_feature_4    91700 non-null  float64\n",
      " 34  chroma_feature_5    91700 non-null  float64\n",
      " 35  chroma_feature_6    91700 non-null  float64\n",
      " 36  chroma_feature_7    91700 non-null  float64\n",
      " 37  chroma_feature_8    91700 non-null  float64\n",
      " 38  chroma_feature_9    91700 non-null  float64\n",
      " 39  chroma_feature_10   91700 non-null  float64\n",
      " 40  chroma_feature_11   91700 non-null  float64\n",
      " 41  chroma_feature_12   91700 non-null  float64\n",
      " 42  chroma_feature_13   91700 non-null  float64\n",
      "dtypes: float64(41), object(2)\n",
      "memory usage: 30.1+ MB\n"
     ]
    }
   ],
   "source": [
    "audio_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Check null values**\n",
    "\n",
    "---\n",
    "\n",
    "This is done to ensure that there is no empty cells on the datasets. \n",
    "Checking for null values in datasets is vital to maintain data integrity and accuracy. Null values can distort analysis results, leading to biased insights and erroneous conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "audio_file            0\n",
       "label                 0\n",
       "zero_crossing_rate    0\n",
       "spectral_centroid     0\n",
       "mfcc_1                0\n",
       "mfcc_2                0\n",
       "mfcc_3                0\n",
       "mfcc_4                0\n",
       "mfcc_5                0\n",
       "mfcc_6                0\n",
       "mfcc_7                0\n",
       "mfcc_8                0\n",
       "mfcc_9                0\n",
       "mfcc_10               0\n",
       "mfcc_11               0\n",
       "mfcc_12               0\n",
       "mfcc_13               0\n",
       "d_mfcc_1              0\n",
       "d_mfcc_2              0\n",
       "d_mfcc_3              0\n",
       "d_mfcc_4              0\n",
       "d_mfcc_5              0\n",
       "d_mfcc_6              0\n",
       "d_mfcc_7              0\n",
       "d_mfcc_8              0\n",
       "d_mfcc_9              0\n",
       "d_mfcc_10             0\n",
       "d_mfcc_11             0\n",
       "d_mfcc_12             0\n",
       "d_mfcc_13             0\n",
       "chroma_feature_1      0\n",
       "chroma_feature_2      0\n",
       "chroma_feature_3      0\n",
       "chroma_feature_4      0\n",
       "chroma_feature_5      0\n",
       "chroma_feature_6      0\n",
       "chroma_feature_7      0\n",
       "chroma_feature_8      0\n",
       "chroma_feature_9      0\n",
       "chroma_feature_10     0\n",
       "chroma_feature_11     0\n",
       "chroma_feature_12     0\n",
       "chroma_feature_13     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing**\n",
    "\n",
    "---\n",
    "\n",
    "In this part of the notebook before going into modelling, we will do data preprocessing with the following steps:\n",
    "1. Set X and Y variable, split the data into train and test data\n",
    "2. Class proportion check and balancing\n",
    "3. Standardscale the X features for both train and test\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Set X and y, Split Train and Test Data**\n",
    "\n",
    "Convert the label column as binary number for modelling purpose\n",
    "\n",
    "We will convert the label of `real` and `fake` as follows:\n",
    "1. `real` = 0\n",
    "2. `fake` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df['label'] = audio_df['label'].apply(lambda x: 0 if x == 'real' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test. `Train` data are used as the dataset to train the model, `test` data are to verify how the perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set X and y variable \n",
    "X = audio_df.drop(columns = ['audio_file','label'])\n",
    "y = audio_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Class Proportion Check and Balancing**\n",
    "\n",
    "---\n",
    "\n",
    "Balancing datasets proportionally aims to address biases caused by unequal class distributions, ensuring that all classes receive equitable representation during model training. By balancing the dataset, models become less prone to favoring majority classes and can better generalize to minority classes, improving overall predictive performance\n",
    "\n",
    "Check the proportion of the target variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.857143\n",
       "0    0.142857\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on proportion of the fake and real, we can see that there is highly imbalance class. \n",
    "Addressing class imbalance is crucial for several reasons. Firstly, imbalanced data can lead to biased model training, where the model tends to favor the majority class and overlook the minority class. Failing to address class imbalance can lead to misleading conclusions and ineffective decision-making based on the model's outputs.\n",
    "\n",
    "We will use `oversampling method (ADASYN)` to balance out the class, which is to amplify the minority class representation (`real`). The reason of not choosing `undersampling method` is that the `fake` data are created by different types of GANs, using undersampling could potentially discard valuable information from the `fake` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create oversample with ADASYN\n",
    "ada = ADASYN(random_state = 42)\n",
    "X_train_resample, y_train_resample = ada.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data proportion after resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.502027\n",
       "1    0.497973\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resample.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are balanced now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. StandardScale Datasets**\n",
    "\n",
    "---\n",
    "\n",
    "StandardScaler is used to standardize features by removing the mean and scaling to unit variance. It transforms the data such that it has a mean of 0 and a standard deviation of 1, making it easier to compare and interpret the effects of different features on machine learning models.\n",
    "\n",
    "We will have an overview of how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>...</th>\n",
       "      <th>chroma_feature_4</th>\n",
       "      <th>chroma_feature_5</th>\n",
       "      <th>chroma_feature_6</th>\n",
       "      <th>chroma_feature_7</th>\n",
       "      <th>chroma_feature_8</th>\n",
       "      <th>chroma_feature_9</th>\n",
       "      <th>chroma_feature_10</th>\n",
       "      <th>chroma_feature_11</th>\n",
       "      <th>chroma_feature_12</th>\n",
       "      <th>chroma_feature_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.144025</td>\n",
       "      <td>1804.273201</td>\n",
       "      <td>-388.87500</td>\n",
       "      <td>93.59637</td>\n",
       "      <td>-34.152460</td>\n",
       "      <td>58.156796</td>\n",
       "      <td>-17.140770</td>\n",
       "      <td>-2.440681</td>\n",
       "      <td>-14.372906</td>\n",
       "      <td>-27.444033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329877</td>\n",
       "      <td>0.328536</td>\n",
       "      <td>0.308672</td>\n",
       "      <td>0.285831</td>\n",
       "      <td>0.325204</td>\n",
       "      <td>0.327426</td>\n",
       "      <td>0.335113</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>0.502557</td>\n",
       "      <td>0.399010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.092919</td>\n",
       "      <td>1374.336393</td>\n",
       "      <td>-356.26257</td>\n",
       "      <td>120.89103</td>\n",
       "      <td>-26.184362</td>\n",
       "      <td>44.641727</td>\n",
       "      <td>-8.690201</td>\n",
       "      <td>-6.371420</td>\n",
       "      <td>-10.669409</td>\n",
       "      <td>-26.607450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340985</td>\n",
       "      <td>0.364688</td>\n",
       "      <td>0.442239</td>\n",
       "      <td>0.421427</td>\n",
       "      <td>0.356675</td>\n",
       "      <td>0.294047</td>\n",
       "      <td>0.288776</td>\n",
       "      <td>0.329846</td>\n",
       "      <td>0.348246</td>\n",
       "      <td>0.363997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093125</td>\n",
       "      <td>1375.684725</td>\n",
       "      <td>-390.94904</td>\n",
       "      <td>125.07535</td>\n",
       "      <td>-26.158548</td>\n",
       "      <td>62.868484</td>\n",
       "      <td>-2.694161</td>\n",
       "      <td>-1.098944</td>\n",
       "      <td>-13.236238</td>\n",
       "      <td>-28.450220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283847</td>\n",
       "      <td>0.320170</td>\n",
       "      <td>0.377925</td>\n",
       "      <td>0.396044</td>\n",
       "      <td>0.402317</td>\n",
       "      <td>0.386759</td>\n",
       "      <td>0.399501</td>\n",
       "      <td>0.392347</td>\n",
       "      <td>0.342137</td>\n",
       "      <td>0.340530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060562</td>\n",
       "      <td>1026.450473</td>\n",
       "      <td>-352.30856</td>\n",
       "      <td>123.55159</td>\n",
       "      <td>-1.788278</td>\n",
       "      <td>21.133470</td>\n",
       "      <td>-10.293497</td>\n",
       "      <td>-17.433174</td>\n",
       "      <td>-17.481981</td>\n",
       "      <td>-9.520210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383526</td>\n",
       "      <td>0.414682</td>\n",
       "      <td>0.431017</td>\n",
       "      <td>0.410287</td>\n",
       "      <td>0.348064</td>\n",
       "      <td>0.293355</td>\n",
       "      <td>0.265428</td>\n",
       "      <td>0.295578</td>\n",
       "      <td>0.313720</td>\n",
       "      <td>0.331562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088338</td>\n",
       "      <td>1290.272797</td>\n",
       "      <td>-374.71390</td>\n",
       "      <td>110.32203</td>\n",
       "      <td>-14.796771</td>\n",
       "      <td>32.152317</td>\n",
       "      <td>-9.729928</td>\n",
       "      <td>-10.600810</td>\n",
       "      <td>-13.255384</td>\n",
       "      <td>-21.119380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271848</td>\n",
       "      <td>0.260946</td>\n",
       "      <td>0.243622</td>\n",
       "      <td>0.242083</td>\n",
       "      <td>0.298031</td>\n",
       "      <td>0.409493</td>\n",
       "      <td>0.383844</td>\n",
       "      <td>0.415713</td>\n",
       "      <td>0.493690</td>\n",
       "      <td>0.503742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   zero_crossing_rate  spectral_centroid     mfcc_1     mfcc_2     mfcc_3  \\\n",
       "0            0.144025        1804.273201 -388.87500   93.59637 -34.152460   \n",
       "1            0.092919        1374.336393 -356.26257  120.89103 -26.184362   \n",
       "2            0.093125        1375.684725 -390.94904  125.07535 -26.158548   \n",
       "3            0.060562        1026.450473 -352.30856  123.55159  -1.788278   \n",
       "4            0.088338        1290.272797 -374.71390  110.32203 -14.796771   \n",
       "\n",
       "      mfcc_4     mfcc_5     mfcc_6     mfcc_7     mfcc_8  ...  \\\n",
       "0  58.156796 -17.140770  -2.440681 -14.372906 -27.444033  ...   \n",
       "1  44.641727  -8.690201  -6.371420 -10.669409 -26.607450  ...   \n",
       "2  62.868484  -2.694161  -1.098944 -13.236238 -28.450220  ...   \n",
       "3  21.133470 -10.293497 -17.433174 -17.481981  -9.520210  ...   \n",
       "4  32.152317  -9.729928 -10.600810 -13.255384 -21.119380  ...   \n",
       "\n",
       "   chroma_feature_4  chroma_feature_5  chroma_feature_6  chroma_feature_7  \\\n",
       "0          0.329877          0.328536          0.308672          0.285831   \n",
       "1          0.340985          0.364688          0.442239          0.421427   \n",
       "2          0.283847          0.320170          0.377925          0.396044   \n",
       "3          0.383526          0.414682          0.431017          0.410287   \n",
       "4          0.271848          0.260946          0.243622          0.242083   \n",
       "\n",
       "   chroma_feature_8  chroma_feature_9  chroma_feature_10  chroma_feature_11  \\\n",
       "0          0.325204          0.327426           0.335113           0.467133   \n",
       "1          0.356675          0.294047           0.288776           0.329846   \n",
       "2          0.402317          0.386759           0.399501           0.392347   \n",
       "3          0.348064          0.293355           0.265428           0.295578   \n",
       "4          0.298031          0.409493           0.383844           0.415713   \n",
       "\n",
       "   chroma_feature_12  chroma_feature_13  \n",
       "0           0.502557           0.399010  \n",
       "1           0.348246           0.363997  \n",
       "2           0.342137           0.340530  \n",
       "3           0.313720           0.331562  \n",
       "4           0.493690           0.503742  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `X_train_resample` dataset, there are significant differences in terms of the values. Using standardscaler will normalized all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply scaling to the datasets \n",
    "ss = StandardScaler()\n",
    "\n",
    "#fit and transform X_train\n",
    "X_train_resample_ss = ss.fit_transform(X_train_resample)\n",
    "\n",
    "#transform X_test\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scaled data, we will move to the next step, `baseline modelling`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Baseline Modelling**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing data preprocessing, the next step is to establish the baseline model. This initial model serves the purpose of identifying which approach holds the most promise for further optimization. By systematically exploring a diverse range of models, we ensure that every potential solution receives consideration, thereby providing a robust groundwork for subsequent model development and refinement efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantitate model for base modelling\n",
    "classifiers = {\n",
    "    'Logistic Regression' : LogisticRegression(),\n",
    "    'Random Forest' : RandomForestClassifier(),\n",
    "    'Gradient Boost' : GradientBoostingClassifier(),\n",
    "    'ADA Boost' : AdaBoostClassifier(),\n",
    "    'Decision Tree' : DecisionTreeClassifier(),\n",
    "    'SVC' : SVC(),\n",
    "    'XG Boost' : XGBClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "Logistic Regression cycle is completed\n",
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "Random Forest cycle is completed\n",
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "Gradient Boost cycle is completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "ADA Boost cycle is completed\n",
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "Decision Tree cycle is completed\n",
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "SVC cycle is completed\n",
      "cross_val_score done\n",
      "train score done\n",
      "test score done\n",
      "XG Boost cycle is completed\n"
     ]
    }
   ],
   "source": [
    "cv_score_list = []\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "#run prediction model \n",
    "for class_name, classifier in classifiers.items(): \n",
    "    #fit model\n",
    "    classifier.fit(X_train_resample_ss, y_train_resample)\n",
    "\n",
    "    #cross validate the score\n",
    "    cv_train_score = cross_val_score(classifier, X_train_resample_ss,y_train_resample, cv =5)\n",
    "    cv_test_score = cross_val_score(classifier,X_test_ss, y_test, cv=5)\n",
    "\n",
    "    #append to cross validate score list\n",
    "    cv_score_list.append({\n",
    "        \"Classifer\" : class_name,\n",
    "        'Train score' : cv_train_score.mean(),\n",
    "        'Test score' : cv_test_score.mean()\n",
    "    })\n",
    "    print('cross_val_score done')\n",
    "\n",
    "    #train scores\n",
    "    train_pred = classifier.predict(X_train_resample_ss)\n",
    "    train_accuracy = accuracy_score(y_train_resample, train_pred)\n",
    "    train_precision = precision_score(y_train_resample, train_pred)\n",
    "    train_recall = recall_score(y_train_resample,train_pred)\n",
    "    train_f1 = f1_score(y_train_resample,train_pred)\n",
    "    train_score_list.append({\n",
    "        'Classifer' : class_name,\n",
    "        'Accuracy' : train_accuracy,\n",
    "        'Precision' : train_precision,\n",
    "        'Recall' : train_recall,\n",
    "        'f1_score' : train_f1\n",
    "    })\n",
    "    print('train score done')\n",
    "\n",
    "    #test scores\n",
    "    test_pred = classifier.predict(X_test_ss)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_precision = precision_score(y_test, test_pred)\n",
    "    test_recall = recall_score(y_test,test_pred)\n",
    "    test_f1 = f1_score(y_test,test_pred)\n",
    "    test_score_list.append({\n",
    "        'Classifer' : class_name,\n",
    "        'Accuracy' : test_accuracy,\n",
    "        'Precision' : test_precision,\n",
    "        'Recall' : test_recall,\n",
    "        'f1_score' : test_f1\n",
    "    })\n",
    "    print('test score done')\n",
    "    print(f'{class_name} cycle is completed')\n",
    "\n",
    "#convert the list to dataframe\n",
    "cv_result_df = pd.DataFrame(cv_score_list)\n",
    "train_result_df = pd.DataFrame(train_score_list)\n",
    "test_result_df = pd.DataFrame(test_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Display the result of each models**\n",
    "\n",
    "---\n",
    "\n",
    "The intention is to compare and select the best baseline model out of 6 classification model to be further hypertuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifer</th>\n",
       "      <th>Train score</th>\n",
       "      <th>Test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.686391</td>\n",
       "      <td>0.859019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.811573</td>\n",
       "      <td>0.855573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.674328</td>\n",
       "      <td>0.857754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA Boost</td>\n",
       "      <td>0.654477</td>\n",
       "      <td>0.855703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.675511</td>\n",
       "      <td>0.757470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.774176</td>\n",
       "      <td>0.857099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.744577</td>\n",
       "      <td>0.849422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Classifer  Train score  Test score\n",
       "0  Logistic Regression     0.686391    0.859019\n",
       "1        Random Forest     0.811573    0.855573\n",
       "2       Gradient Boost     0.674328    0.857754\n",
       "3            ADA Boost     0.654477    0.855703\n",
       "4        Decision Tree     0.675511    0.757470\n",
       "5                  SVC     0.774176    0.857099\n",
       "6             XG Boost     0.744577    0.849422"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display cross_val_score result \n",
    "cv_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.687684</td>\n",
       "      <td>0.691060</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>0.682551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.693276</td>\n",
       "      <td>0.703429</td>\n",
       "      <td>0.664003</td>\n",
       "      <td>0.683148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA Boost</td>\n",
       "      <td>0.664842</td>\n",
       "      <td>0.669636</td>\n",
       "      <td>0.645327</td>\n",
       "      <td>0.657256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.835226</td>\n",
       "      <td>0.855967</td>\n",
       "      <td>0.804478</td>\n",
       "      <td>0.829424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.863674</td>\n",
       "      <td>0.809788</td>\n",
       "      <td>0.835863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Classifer  Accuracy  Precision    Recall  f1_score\n",
       "0  Logistic Regression  0.687684   0.691060  0.674249  0.682551\n",
       "1        Random Forest  1.000000   1.000000  1.000000  1.000000\n",
       "2       Gradient Boost  0.693276   0.703429  0.664003  0.683148\n",
       "3            ADA Boost  0.664842   0.669636  0.645327  0.657256\n",
       "4        Decision Tree  1.000000   1.000000  1.000000  1.000000\n",
       "5                  SVC  0.835226   0.855967  0.804478  0.829424\n",
       "6             XG Boost  0.841629   0.863674  0.809788  0.835863"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display train result\n",
    "train_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.674329</td>\n",
       "      <td>0.926850</td>\n",
       "      <td>0.673181</td>\n",
       "      <td>0.779907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.727895</td>\n",
       "      <td>0.852836</td>\n",
       "      <td>0.824885</td>\n",
       "      <td>0.838628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boost</td>\n",
       "      <td>0.644057</td>\n",
       "      <td>0.901748</td>\n",
       "      <td>0.656234</td>\n",
       "      <td>0.759647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA Boost</td>\n",
       "      <td>0.631799</td>\n",
       "      <td>0.897849</td>\n",
       "      <td>0.643664</td>\n",
       "      <td>0.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.622334</td>\n",
       "      <td>0.856051</td>\n",
       "      <td>0.672468</td>\n",
       "      <td>0.753235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.726107</td>\n",
       "      <td>0.899158</td>\n",
       "      <td>0.766412</td>\n",
       "      <td>0.827495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XG Boost</td>\n",
       "      <td>0.695921</td>\n",
       "      <td>0.890965</td>\n",
       "      <td>0.735216</td>\n",
       "      <td>0.805632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Classifer  Accuracy  Precision    Recall  f1_score\n",
       "0  Logistic Regression  0.674329   0.926850  0.673181  0.779907\n",
       "1        Random Forest  0.727895   0.852836  0.824885  0.838628\n",
       "2       Gradient Boost  0.644057   0.901748  0.656234  0.759647\n",
       "3            ADA Boost  0.631799   0.897849  0.643664  0.749800\n",
       "4        Decision Tree  0.622334   0.856051  0.672468  0.753235\n",
       "5                  SVC  0.726107   0.899158  0.766412  0.827495\n",
       "6             XG Boost  0.695921   0.890965  0.735216  0.805632"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display test result\n",
    "test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select which models perform the best, we evaluate based on the confusion matrix.\n",
    "Below is the confusion matrix breakdown:\n",
    "\n",
    "1. True Positive: Predict that the audio is fake, actual is fake\n",
    "\n",
    "2. True Negative: Predict that the audio is not fake, actual is not fake\n",
    "\n",
    "3. False Positive: Predict that the audio is fake, actual is not fake \n",
    "\n",
    "4. False Negative: Predict that the audio is not fake, actual is fake\n",
    "\n",
    "Our primary metric of concern is `recall`, as it directly relates to minimizing `false negatives`. `False negatives` occur when our model predicts that an audio clip is genuine when it is actually fake. This poses a significant risk to our users, as they may trust the authenticity of the audio when they should not. For instance, in scenarios involving scams, a high `false negative` could result in our users being deceived into believing that the audio is legitimate, leading to potential financial losses or other harm. Therefore, our goal is to minimize `false negatives` in order to enhance the overall reliability and safety of our system.\n",
    "\n",
    "Considering our objective of minimizing false negatives in classification models, we focus on evaluating the `recall` score. Maximizing the `recall` score effectively reduces the occurrence of `false negatives` which aligns with our goal of enhancing model performance and mitigating the risk associated with incorrectly identifying fake audio clips as real\n",
    "\n",
    "Based on the model above, we will pick top 3 model by evaluating the highest recall score.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Selected models:**\n",
    "1. `Random Forest`\n",
    "2. `Logistic Regression`\n",
    "3. `XG Boost`\n",
    "\n",
    "We will proceed to the next notebook for hypertuning.\n",
    "\n",
    "---\n",
    "\n",
    "Next : [03 Hyperparameter Tuning of Baseline Model](03_hyperparametertuning_traditional_model.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
